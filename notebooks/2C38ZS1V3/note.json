{
  "paragraphs": [
    {
      "text": "sc",
      "dateUpdated": "Nov 17, 2016 6:09:15 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {},
          "map": {
            "baseMapType": "Streets",
            "isOnline": true,
            "pinCols": []
          }
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1479362937029_92806956",
      "id": "20161117-060857_423767350",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "\nres0: org.apache.spark.SparkContext \u003d org.apache.spark.SparkContext@6441be14\n"
      },
      "dateCreated": "Nov 17, 2016 6:08:57 AM",
      "dateStarted": "Nov 17, 2016 6:09:16 AM",
      "dateFinished": "Nov 17, 2016 6:10:35 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val sqlContext \u003d new org.apache.spark.sql.hive.HiveContext(sc)",
      "dateUpdated": "Nov 17, 2016 8:14:41 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {},
          "map": {
            "baseMapType": "Streets",
            "isOnline": true,
            "pinCols": []
          }
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1479362955858_-1365376052",
      "id": "20161117-060915_1556351634",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "\n\n\n\u003cconsole\u003e:27: error: object hive is not a member of package org.apache.spark.sql\n       val sqlContext \u003d new org.apache.spark.sql.hive.HiveContext(sc)\n                                                 ^\n"
      },
      "dateCreated": "Nov 17, 2016 6:09:15 AM",
      "dateStarted": "Nov 17, 2016 8:14:41 AM",
      "dateFinished": "Nov 17, 2016 8:16:00 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "sqlContext.sql(\"show tables\")",
      "dateUpdated": "Nov 17, 2016 8:17:23 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {},
          "map": {
            "baseMapType": "Streets",
            "isOnline": true,
            "pinCols": []
          }
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1479370481802_541111930",
      "id": "20161117-081441_426834194",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\norg.apache.spark.SparkException: Unable to create database default as failed to create its directory hdfs://spark-docker:9000/table\n  at org.apache.spark.sql.catalyst.catalog.InMemoryCatalog.liftedTree1$1(InMemoryCatalog.scala:114)\n  at org.apache.spark.sql.catalyst.catalog.InMemoryCatalog.createDatabase(InMemoryCatalog.scala:108)\n  at org.apache.spark.sql.catalyst.catalog.SessionCatalog.createDatabase(SessionCatalog.scala:147)\n  at org.apache.spark.sql.catalyst.catalog.SessionCatalog.\u003cinit\u003e(SessionCatalog.scala:89)\n  at org.apache.spark.sql.internal.SessionState.catalog$lzycompute(SessionState.scala:95)\n  at org.apache.spark.sql.internal.SessionState.catalog(SessionState.scala:95)\n  at org.apache.spark.sql.internal.SessionState$$anon$1.\u003cinit\u003e(SessionState.scala:112)\n  at org.apache.spark.sql.internal.SessionState.analyzer$lzycompute(SessionState.scala:112)\n  at org.apache.spark.sql.internal.SessionState.analyzer(SessionState.scala:111)\n  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:49)\n  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:64)\n  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:582)\n  at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:682)\n  ... 47 elided\nCaused by: java.net.ConnectException: Call From 8126ffe1c7e2/172.21.0.4 to spark-docker:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\n  at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n  at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n  at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n  at java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n  at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)\n  at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)\n  at org.apache.hadoop.ipc.Client.call(Client.java:1479)\n  at org.apache.hadoop.ipc.Client.call(Client.java:1412)\n  at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)\n  at com.sun.proxy.$Proxy15.mkdirs(Unknown Source)\n  at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:558)\n  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n  at java.lang.reflect.Method.invoke(Method.java:498)\n  at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)\n  at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n  at com.sun.proxy.$Proxy16.mkdirs(Unknown Source)\n  at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:3000)\n  at org.apache.hadoop.hdfs.DFSClient.mkdirs(DFSClient.java:2970)\n  at org.apache.hadoop.hdfs.DistributedFileSystem$21.doCall(DistributedFileSystem.java:1047)\n  at org.apache.hadoop.hdfs.DistributedFileSystem$21.doCall(DistributedFileSystem.java:1043)\n  at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n  at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirsInternal(DistributedFileSystem.java:1043)\n  at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirs(DistributedFileSystem.java:1036)\n  at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:1877)\n  at org.apache.spark.sql.catalyst.catalog.InMemoryCatalog.liftedTree1$1(InMemoryCatalog.scala:111)\n  ... 59 more\nCaused by: java.net.ConnectException: Connection refused\n  at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n  at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n  at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)\n  at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)\n  at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)\n  at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)\n  at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)\n  at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)\n  at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)\n  at org.apache.hadoop.ipc.Client.call(Client.java:1451)\n  ... 79 more\n"
      },
      "dateCreated": "Nov 17, 2016 8:14:41 AM",
      "dateStarted": "Nov 17, 2016 8:17:23 AM",
      "dateFinished": "Nov 17, 2016 8:17:42 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "sqlContext.sql(\"select * from account\")",
      "dateUpdated": "Nov 17, 2016 8:22:00 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {},
          "map": {
            "baseMapType": "Streets",
            "isOnline": true,
            "pinCols": []
          }
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1479370643342_-208196682",
      "id": "20161117-081723_31494287",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\norg.apache.spark.SparkException: Unable to create database default as failed to create its directory hdfs://spark-docker:9000/table\n  at org.apache.spark.sql.catalyst.catalog.InMemoryCatalog.liftedTree1$1(InMemoryCatalog.scala:114)\n  at org.apache.spark.sql.catalyst.catalog.InMemoryCatalog.createDatabase(InMemoryCatalog.scala:108)\n  at org.apache.spark.sql.catalyst.catalog.SessionCatalog.createDatabase(SessionCatalog.scala:147)\n  at org.apache.spark.sql.catalyst.catalog.SessionCatalog.\u003cinit\u003e(SessionCatalog.scala:89)\n  at org.apache.spark.sql.internal.SessionState.catalog$lzycompute(SessionState.scala:95)\n  at org.apache.spark.sql.internal.SessionState.catalog(SessionState.scala:95)\n  at org.apache.spark.sql.internal.SessionState$$anon$1.\u003cinit\u003e(SessionState.scala:112)\n  at org.apache.spark.sql.internal.SessionState.analyzer$lzycompute(SessionState.scala:112)\n  at org.apache.spark.sql.internal.SessionState.analyzer(SessionState.scala:111)\n  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:49)\n  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:64)\n  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:582)\n  at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:682)\n  ... 47 elided\nCaused by: java.net.ConnectException: Call From 8126ffe1c7e2/172.21.0.4 to spark-docker:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\n  at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n  at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n  at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n  at java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n  at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)\n  at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)\n  at org.apache.hadoop.ipc.Client.call(Client.java:1479)\n  at org.apache.hadoop.ipc.Client.call(Client.java:1412)\n  at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)\n  at com.sun.proxy.$Proxy15.mkdirs(Unknown Source)\n  at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:558)\n  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n  at java.lang.reflect.Method.invoke(Method.java:498)\n  at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)\n  at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n  at com.sun.proxy.$Proxy16.mkdirs(Unknown Source)\n  at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:3000)\n  at org.apache.hadoop.hdfs.DFSClient.mkdirs(DFSClient.java:2970)\n  at org.apache.hadoop.hdfs.DistributedFileSystem$21.doCall(DistributedFileSystem.java:1047)\n  at org.apache.hadoop.hdfs.DistributedFileSystem$21.doCall(DistributedFileSystem.java:1043)\n  at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n  at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirsInternal(DistributedFileSystem.java:1043)\n  at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirs(DistributedFileSystem.java:1036)\n  at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:1877)\n  at org.apache.spark.sql.catalyst.catalog.InMemoryCatalog.liftedTree1$1(InMemoryCatalog.scala:111)\n  ... 59 more\nCaused by: java.net.ConnectException: Connection refused\n  at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n  at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n  at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)\n  at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)\n  at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)\n  at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)\n  at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)\n  at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)\n  at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)\n  at org.apache.hadoop.ipc.Client.call(Client.java:1451)\n  ... 79 more\n"
      },
      "dateCreated": "Nov 17, 2016 8:17:23 AM",
      "dateStarted": "Nov 17, 2016 8:22:00 AM",
      "dateFinished": "Nov 17, 2016 8:22:03 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "",
      "dateUpdated": "Nov 17, 2016 8:21:34 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {},
          "map": {
            "baseMapType": "Streets",
            "isOnline": true,
            "pinCols": []
          }
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1479370883234_-789027435",
      "id": "20161117-082123_2101476496",
      "dateCreated": "Nov 17, 2016 8:21:23 AM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "test1",
  "id": "2C38ZS1V3",
  "angularObjects": {
    "2C2JM1Q6B:shared_process": [],
    "2C2F89TDT:shared_process": [],
    "2C41E3TFN:shared_process": [],
    "2BZZCMM2T:shared_process": [],
    "2C15JGEDV:shared_process": [],
    "2C3U2XXRB:shared_process": [],
    "2C2CYD7ED:shared_process": [],
    "2C23JHBJQ:shared_process": [],
    "2C2C1T3DG:shared_process": []
  },
  "config": {},
  "info": {}
}